{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Setup Scrapy project using Scrapy CLI\n",
    "* Review Folder Structure\n",
    "* Add Spider to the Project\n",
    "* Update Settings to write to json file\n",
    "* Run and Validate the Project\n",
    "* Exercise and Solution - Scrape Data to JSON Files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Setting up Project using Scrapy CLI\n",
    "\n",
    "1. Run `scrapy startproject quotes` to setup scrapy project.\n",
    "2. Check whether a folder by name quotes is not created or not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Review Folder Structure\n",
    "\n",
    "1. Check configuration file\n",
    "2. Review `spiders` folder\n",
    "3. Review `settings.py`\n",
    "4. Review `pipelines.py`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add Spider to the Project\n",
    "\n",
    "1. Add a program file under `quotes/quotes/spiders` folder by name quotes_spider.py\n",
    "2. Review and understand the code. The code have the ability to process the quotes in all the 100 pages that are available under base url - https://www.goodreads.com/quotes\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "def generate_urls(base_url):\n",
    "    urls = []\n",
    "    for i in range(1, 101):\n",
    "        urls.append(f'{base_url}?page={i}')\n",
    "    return urls\n",
    "\n",
    "    \n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = generate_urls('https://www.goodreads.com/quotes')\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quoteDetails in response.css('.quoteDetails'):\n",
    "            payload = {\n",
    "                'quoteText': quoteDetails.css('.quoteText::text').get(),\n",
    "                'authorOrTitle': quoteDetails.css('span.authorOrTitle::text').get()\n",
    "            }\n",
    "            yield payload\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using follow to navigate via next page\n",
    "\n",
    "You can also develop the logic to scrape all the pages by following using next_page.\n",
    "\n",
    "```python\n",
    "import hashlib\n",
    "import scrapy\n",
    "\n",
    "    \n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = ['https://www.goodreads.com/quotes']\n",
    "\n",
    "    def parse(self, response):\n",
    "        sha = hashlib.sha256()\n",
    "        for quoteDetails in response.css('.quoteDetails'):\n",
    "            quote_text = quoteDetails.css('.quoteText::text').get()\n",
    "            sha.update(quote_text.encode())\n",
    "            payload = {\n",
    "                'quoteTextHash': sha.hexdigest(),\n",
    "                'quoteText': quote_text,\n",
    "                'authorOrTitle': quoteDetails.css('span.authorOrTitle::text').get(),\n",
    "                'authorOrTitleUrl': quoteDetails.css('a.authorOrTitle::attr(href)').get(),\n",
    "                'authorOrTitleText': quoteDetails.css('a.authorOrTitle::text').get()\n",
    "            }\n",
    "            yield payload\n",
    "\n",
    "        for next_page in response.css('a.next_page'):\n",
    "                if response.css('a.disabled'):\n",
    "                     break\n",
    "                yield response.follow(next_page, self.parse)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Update Settings to write to json file\n",
    "\n",
    "1. Go to `settings.py` in scrapy project folder.\n",
    "2. Append below text to the file and save it. You can also specify the full path for the output file.\n",
    "\n",
    "```python\n",
    "FEEDS = {\n",
    "    'quotes.json': {\n",
    "        'format': 'json',\n",
    "        'overwrite': True\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run and Validate the Project\n",
    "\n",
    "1. Run `scrapy crawl quotes` to run the spider in the spider project.\n",
    "2. Review the data in the files.\n",
    "\n",
    "Note: If you run the project multiple times, the file will be overwritten."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exercise  - Scrape Data to JSON Files\n",
    "\n",
    "Scrape quote text, author or title, author or title url, author or title url text into JSON format.\n",
    "\n",
    "1. Make sure to have a project by name quotes.\n",
    "2. Define spider to scrape all the 100 pages.\n",
    "3. Save quote text, author or title, author or title url, and author or title url text details to json file to a file by name `quotes.json`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Solution - Scrape Data to JSON Files\n",
    "\n",
    "Scrape quote text, author or title, author or title url, author or title url text into JSON format.\n",
    "\n",
    "1. Make sure to have a project by name quotes.\n",
    "\n",
    "```shell\n",
    "scrapy startproject quotes\n",
    "```\n",
    "\n",
    "2. Define spider to scrape all the 100 pages.\n",
    "\n",
    "Create a file by name quotes_spider.py in spiders folder.\n",
    "\n",
    "3. Save quote text, author or title, author or title url, and author or title url text details to json file to a file by name `quotes.json`\n",
    "\n",
    "Update `quotes_spider.py` with below code.\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "import json\n",
    "\n",
    "\n",
    "def generate_urls(base_url):\n",
    "    urls = []\n",
    "    for i in range(1, 101):\n",
    "        urls.append(f'{base_url}?page={i}')\n",
    "    return urls\n",
    "\n",
    "    \n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = generate_urls('https://www.goodreads.com/quotes')\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quoteDetails in response.css('.quoteDetails'):\n",
    "            payload = {\n",
    "                'quoteText': quoteDetails.css('.quoteText::text').get(),\n",
    "                'authorOrTitle': quoteDetails.css('span.authorOrTitle::text').get(),\n",
    "                'authorOrTitleUrl': quoteDetails.css('a.authorOrTitle::attr(href)').get(),\n",
    "                'authorOrTitleText': quoteDetails.css('a.authorOrTitle::text').get()\n",
    "            }\n",
    "            yield payload\n",
    "```\n",
    "\n",
    "Make sure below setting is added to `settings.py`.\n",
    "\n",
    "```python\n",
    "FEEDS = {\n",
    "    'quotes.json': {\n",
    "        'format': 'json',\n",
    "        'overwrite': True\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Run `scrapy crawl quotes` to add the data to `quotes.json`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
