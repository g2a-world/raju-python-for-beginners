{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Install Scrapy for Web Scraping\n",
    "* Review the Code of the first spider\n",
    "* Run and Validate the Application\n",
    "* Review Website to be scraped\n",
    "* Create Spider to read quotes\n",
    "* Update logic to get urls for authors or titles\n",
    "* Overview of writing data to json\n",
    "* Exercise and Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Install Scrapy for Web Scraping\n",
    "\n",
    "You can use official documentation at https://scrapy.org to quickly install and start using it.\n",
    "\n",
    "1. Install scrapy using `python -m pip install scrapy==2.8.0`. You can also install latest by saying `python -m pip install scrapy`. \n",
    "2. In the base folder add a file by name `myspider.py`\n",
    "3. Copy paste this code and save the file. A new spider by name myspider will be created to scrape a given website.\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class BlogSpider(scrapy.Spider):\n",
    "    name = 'blogspider'\n",
    "    start_urls = ['https://www.zyte.com/blog/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for title in response.css('.oxy-post-title'):\n",
    "            yield {'title': title.css('::text').get()}\n",
    "\n",
    "        for next_page in response.css('a.next'):\n",
    "            yield response.follow(next_page, self.parse)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Review the Code of the first spider\n",
    "\n",
    "1. Make sure to go to the specified URL in `myspider.py` and understand how blog pages are organized.\n",
    "2. When we run `myspider`, the logic in the `parse` will be executed for the urls specified under `start_urls` list.\n",
    "3. First `for` loop gives us the titles in each blob post or page.\n",
    "4. The blog posts/pages in the specified URL are paginated. The second for loop will take care of going to each page and then parse.\n",
    "\n",
    "In the end we will get all the blog post/page titles from the website."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run and Validate the Application\n",
    "\n",
    "1. Installing Scrapy will also provide us `scrapy` CLI. Review `scrapy` CLI. Here are the commands to get started with scrapy and run spider.\n",
    "\n",
    "```shell\n",
    "scrapy version\n",
    "scrapy runspider myspider.py\n",
    "```\n",
    "\n",
    "2. Use `scrapy` CLI to run the myspider - `scrapy runspider myspider.py`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Review Website to be scraped\n",
    "\n",
    "Go to https://www.goodreads.com/quotes and review the details.\n",
    "\n",
    "1. Visit the website and understand how quote are organized.\n",
    "2. Go to the source code of the HTML page and review the tags related to quotes (quotes, quoteDetails, quoteText, etc.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create Spider to read quotes\n",
    "\n",
    "1. Create new Python program file by name `quotes_spider.py`.\n",
    "2. Add the below code to read the quotes and save.\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = ['https://www.goodreads.com/quotes']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quoteDetails in response.css('.quoteDetails'):\n",
    "            payload = {\n",
    "                'quoteText': quoteDetails.css('.quoteText::text').get()\n",
    "            }\n",
    "            yield payload\n",
    "\n",
    "        for next_page in response.css('a.next_page'):\n",
    "            yield response.follow(next_page, self.parse)\n",
    "```\n",
    "\n",
    "3. Run and validate to get the quote text from each of the quote."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Update logic to get urls for authors or titles\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = ['https://www.goodreads.com/quotes']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quoteDetails in response.css('.quoteDetails'):\n",
    "            payload = {\n",
    "                'quoteText': quoteDetails.css('.quoteText::text').get(),\n",
    "                'authorOrTitle': quoteDetails.css('a.authorOrTitle::attr(href)').get()\n",
    "            }\n",
    "            yield payload\n",
    "\n",
    "        for next_page in response.css('a.next_page'):\n",
    "            yield response.follow(next_page, self.parse)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Overview of writing data to json\n",
    "\n",
    "```python\n",
    "import json\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = ['https://www.goodreads.com/quotes?page=95']\n",
    "\n",
    "    def __init__(self):\n",
    "        self.file = open('quotes.json', 'w')\n",
    "\n",
    "    def closed(self, reason):\n",
    "        self.file.close()\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quoteDetails in response.css('.quoteDetails'):\n",
    "            payload = {\n",
    "                'quoteText': quoteDetails.css('.quoteText::text').get(),\n",
    "                'authorOrTitleUrl': quoteDetails.css('a.authorOrTitle::attr(href)').get()\n",
    "            }\n",
    "            self.file.write(f'{json.dumps(payload)}\\n')\n",
    "            yield payload\n",
    "\n",
    "        for next_page in response.css('a.next_page'):\n",
    "            yield response.follow(next_page, self.parse)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exercise - Get URLs from the wiki page\n",
    "\n",
    "1. Review the content of Wiki Page - https://en.wikipedia.org/wiki/Python_(programming_language)\n",
    "2. Develop required logic to get all the external urls for the above Wiki Page. Make sure to get only those URLs which start with **http**.\n",
    "3. Run and Validate to see if the URLs are retrieved or not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Solution - Get URLs from the wiki page\n",
    "\n",
    "1. Review the content of Wiki Page - https://en.wikipedia.org/wiki/Python_(programming_language)\n",
    "2. Develop required logic to get all the external urls for the above Wiki Page. Make sure to get only those URLs which start with **http**.\n",
    "\n",
    "Add the logic to `wiki_spider.py`\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class WikiSpider(scrapy.Spider):\n",
    "    name = 'wikispider'\n",
    "    start_urls = ['https://en.wikipedia.org/wiki/Python_(programming_language)']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for link_tag in response.css('link'):\n",
    "            url = link_tag.css('::attr(href)').get()\n",
    "            if url and url.startswith('http'):\n",
    "                yield {'url': url}\n",
    "        \n",
    "        for atag in response.css('a'):\n",
    "            url = atag.css('::attr(href)').get()\n",
    "            if url and url.startswith('http'):\n",
    "                yield {'url': url}\n",
    "```\n",
    "\n",
    "3. Run and Validate to see if the URLs are retrieved or not.\n",
    "\n",
    "```shell\n",
    "scrapy runspider wiki_spider.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
