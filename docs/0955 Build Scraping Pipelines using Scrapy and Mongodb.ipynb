{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Validate Mongodb Database Setup\n",
    "* Overview of Scrapy Pipelines\n",
    "* Overview of using hash code for quote text\n",
    "* Update Spider Logic to include hash code\n",
    "* Develop Pipeline Logic to write to Mongodb\n",
    "* Run the Pipeline to write to Mongodb\n",
    "* Validate Data in Mongodb Collection\n",
    "* Exercise and Solution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Validate Mongodb Database Setup\n",
    "\n",
    "1. Make sure Mongodb is running (Use telnet to validate - `telnet localhost 27017`)\n",
    "2. Launch Mongo shell using `mongosh`.\n",
    "3. We can also use `pymongo` to connect to Mongodb Database using Python.\n",
    "\n",
    "```python\n",
    "import pymongo\n",
    "client = pymongo.MongoClient('localhost', 27017)\n",
    "\n",
    "for db in client.list_databases():\n",
    "    print(db['name'])\n",
    "\n",
    "# We can create new database and then use relevant APIs to deal with collections and documents\n",
    "db = client['quotes_db']\n",
    "\n",
    "# If the database is empty, you will not see any collections\n",
    "for collection in db.list_collections():\n",
    "    print(collection)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Overview of Scrapy Pipelines\n",
    "\n",
    "Here are the details about Scrapy Pipelines.\n",
    "1. We can define pipelines in `pipelines.py`.\n",
    "2. The pipeline class will have the logic to write the data to specified target.\n",
    "3. The logic to process HTML content and write to the target such as database are clearly separated.\n",
    "\n",
    "We will understand how to write the extracted data into Mongo DB database using Scrapy pipelines."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Overview of using hash code for quote text\n",
    "\n",
    "```python\n",
    "import hashlib\n",
    "\n",
    "s = 'Hello World'\n",
    "hashlib.md5(s.encode()).hexdigest()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Update Spider Logic to include hash code\n",
    "\n",
    "```python\n",
    "import hashlib\n",
    "import scrapy\n",
    "\n",
    "    \n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "\n",
    "    def start_requests(self):\n",
    "        \"\"\"Special method in place of start urls. This will be called automatically to get the list of urls\"\"\"\n",
    "\n",
    "        def generate_urls(base_url):\n",
    "            urls = []\n",
    "            for i in range(1, 4): # considers 3 pages\n",
    "                urls.append(f'{base_url}?page={i}')\n",
    "            return urls\n",
    "        \n",
    "        urls = generate_urls('https://www.goodreads.com/quotes')\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quoteDetails in response.css('.quoteDetails'):\n",
    "            quote_text = quoteDetails.css('.quoteText::text').get()\n",
    "            payload = {\n",
    "                'quoteTextHash': hashlib.md5(quote_text.encode()).hexdigest(),\n",
    "                'quoteText': quote_text,\n",
    "                'authorOrTitle': quoteDetails.css('span.authorOrTitle::text').get(),\n",
    "                'authorOrTitleUrl': quoteDetails.css('a.authorOrTitle::attr(href)').get(),\n",
    "                'authorOrTitleUrlText': quoteDetails.css('a.authorOrTitle::text').get()\n",
    "            }\n",
    "            yield payload\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Develop Pipeline Logic to write to Mongodb\n",
    "\n",
    "1. Connect to MongoDB Database\n",
    "2. Process the data and store into MongoDB Database\n",
    "3. Close the connection to MongoDB Database\n",
    "\n",
    "Update `pipelines.py`\n",
    "\n",
    "```python\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "class QuotesScraperPipeline:\n",
    "    def __init__(self, mongo_uri, mongo_db, collection_name):\n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.mongo_db = mongo_db\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        mongo_uri = crawler.settings.get('MONGO_URI')\n",
    "        mongo_db = crawler.settings.get('MONGO_DATABASE')\n",
    "        collection_name = crawler.settings.get('MONGO_COLLECTION')\n",
    "        return cls(mongo_uri, mongo_db, collection_name)\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.client = MongoClient(self.mongo_uri)\n",
    "        self.db = self.client[self.mongo_db]\n",
    "        self.collection = self.db[self.collection_name]\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        self.collection.insert_one(dict(item))\n",
    "        return item\n",
    "```\n",
    "\n",
    "Update `settings.py` with Mongo DB connectivity information and also pipeline details. Make sure to comment out or delete the code related to `FEEDS` which will add data to file.\n",
    "\n",
    "```python\n",
    "ITEM_PIPELINES = {\n",
    "    'quotes_scraper.pipelines.QuotesScraperPipeline': 300\n",
    "}\n",
    "\n",
    "MONGO_URI = 'mongodb://localhost:27017/'\n",
    "MONGO_DATABASE = 'quotes_db'\n",
    "MONGO_COLLECTION = 'quotes'\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run the Pipeline to write to Mongodb\n",
    "\n",
    "Run the pipeline using `scrapy crawl quotes`. It will process the data from the specified urls and load the data into Mongo DB collection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Validate Data in Mongodb Collection\n",
    "\n",
    "1. Launch Mongo Shell\n",
    "2. Switch to quotes_db using `use quotes_db`.\n",
    "3. Check the count in the collection using `db.quotes.countDocuments({})`\n",
    "4. Get first few records using pretty `db.quotes.find({}).pretty()`\n",
    "5. Delete data from Mongo Collection using `db.quotes.deleteMany({})`\n",
    "\n",
    "\n",
    "Here are the Python code snippets to validate and delete the data from the collection\n",
    "\n",
    "```python\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "\n",
    "for item in client.list_databases():\n",
    "    print(item['name'])\n",
    "\n",
    "db = client['quotes_db']\n",
    "\n",
    "for coll in db.list_collections():\n",
    "    print(coll['name'])\n",
    "\n",
    "quotes_coll = db['quotes']\n",
    "\n",
    "quotes_coll.count_documents({})\n",
    "\n",
    "quotes_coll.find({})\n",
    "\n",
    "for coll_item in quotes_coll.find({}):\n",
    "    print(coll_item)\n",
    "\n",
    "quotes_coll.delete_many({})\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exercise - Include page urls while writing to Mongodb\n",
    "\n",
    "1. Ensure you add the logic related to adding page urls to the `parse` function. The attribute name should be `parseUrl`. It can be populated using `response.url`. Make sure to have it after `quoteTextHash`.\n",
    "2. Make sure data is upserted or merged. If there is no record in mongodb with given quoteTextHash, then the document should be inserted otherwise document should be updated.\n",
    "3. Validate by reviewing the data in the Mongodb collection.\n",
    "\n",
    "Here is the sample code to upsert data into Mongo collection;\n",
    "\n",
    "```python\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('mongodb://localhost:27017')\n",
    "\n",
    "db = client['demo']\n",
    "\n",
    "coll = db['users']\n",
    "\n",
    "coll.insert_one({\n",
    "    \"user_id\": 1, \n",
    "    \"first_name\": \"Scott\", \n",
    "    \"last_name\": \"Tiger\", \n",
    "    \"username\": \"stiger\", \n",
    "    \"email\": None\n",
    "})\n",
    "\n",
    "coll.insert_one({\n",
    "    \"user_id\": 2, \n",
    "    \"first_name\": \"Donald\", \n",
    "    \"last_name\": \"Duck\", \n",
    "    \"username\": \"dduck\", \n",
    "    \"email\": None\n",
    "})\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "for item in coll.find({}):\n",
    "    pprint(item)\n",
    "\n",
    "coll.update_one(\n",
    "    {\"user_id\": 1},\n",
    "    {\"$set\": {\n",
    "        \"email\": \"stiger@email.com\"\n",
    "    }},\n",
    "    upsert=True\n",
    ")\n",
    "\n",
    "for item in coll.find({}):\n",
    "    pprint(item)\n",
    "\n",
    "query = {'user_id': 3}\n",
    "update = {'$set': {\n",
    "    'first_name': 'Mickey',\n",
    "    'last_name': 'Mouse',\n",
    "    'username': 'mmouse',\n",
    "    'email': 'mmouse@email.com'\n",
    "}}\n",
    "coll.update_one(filter=query, update=update, upsert=True)\n",
    "\n",
    "for item in coll.find({}):\n",
    "    pprint(item)\n",
    "\n",
    "coll.drop()\n",
    "```\n",
    "\n",
    "Here are the equivalent mongo shell commands:\n",
    "\n",
    "```js\n",
    "use demo\n",
    "\n",
    "db.users.insertOne({\n",
    "    \"user_id\": 1, \n",
    "    \"first_name\": \"Scott\", \n",
    "    \"last_name\": \"Tiger\", \n",
    "    \"username\": \"stiger\", \n",
    "    \"email\": null\n",
    "})\n",
    "\n",
    "db.users.insertOne({\n",
    "    \"user_id\": 2, \n",
    "    \"first_name\": \"Donald\", \n",
    "    \"last_name\": \"Duck\", \n",
    "    \"username\": \"dduck\", \n",
    "    \"email\": null\n",
    "})\n",
    "\n",
    "db.users.updateOne(\n",
    "    {\"user_id\": 1}, # query\n",
    "    {\"$set\": {\n",
    "        \"email\": \"stiger@email.com\"\n",
    "    }}, # update\n",
    "    {\"upsert\": true} # update or insert\n",
    ")\n",
    "\n",
    "db.users.updateOne(\n",
    "    {\"user_id\": 3},\n",
    "    {\"$set\": {\n",
    "        \"first_name\": \"Mickey\",\n",
    "        \"last_name\": \"Mouse\",\n",
    "        \"username\": \"mmouse\",\n",
    "        \"email\": \"mmouse@email.com\"\n",
    "    }},\n",
    "    {\"upsert\": true}\n",
    ")\n",
    "\n",
    "db.users.find().pretty()\n",
    "\n",
    "db.users.drop()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Solution - Include page urls while writing to Mongodb\n",
    "\n",
    "1. Update `parse` function in `quotes_spider.py`\n",
    "\n",
    "```python\n",
    "import hashlib\n",
    "import scrapy\n",
    "\n",
    "    \n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = ['https://www.goodreads.com/quotes?page=90']\n",
    "\n",
    "    def parse(self, response):\n",
    "        sha = hashlib.sha256()\n",
    "        for quoteDetails in response.css('.quoteDetails'):\n",
    "            quote_text = quoteDetails.css('.quoteText::text').get()\n",
    "            sha.update(quote_text.encode())\n",
    "            payload = {\n",
    "                'quoteTextHash': sha.hexdigest(),\n",
    "                'pageUrl': response.url,\n",
    "                'quoteText': quote_text,\n",
    "                'authorOrTitle': quoteDetails.css('span.authorOrTitle::text').get(),\n",
    "                'authorOrTitleUrl': quoteDetails.css('a.authorOrTitle::attr(href)').get(),\n",
    "                'authorOrTitleText': quoteDetails.css('a.authorOrTitle::text').get()\n",
    "            }\n",
    "            yield payload\n",
    "\n",
    "        for next_page in response.css('a.next_page'):\n",
    "                yield response.follow(next_page, self.parse)\n",
    "```\n",
    "\n",
    "2. Update `pipelines.py` with required changes to upsert into Mongodb collection\n",
    "\n",
    "```python\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "class QuotesPipeline:\n",
    "    def __init__(self, mongo_uri, mongo_db, collection_name):\n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.mongo_db = mongo_db\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        mongo_uri = crawler.settings.get('MONGO_URI')\n",
    "        mongo_db = crawler.settings.get('MONGO_DATABASE')\n",
    "        collection_name = crawler.settings.get('MONGO_COLLECTION')\n",
    "        return cls(mongo_uri, mongo_db, collection_name)\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.client = MongoClient(self.mongo_uri)\n",
    "        self.db = self.client[self.mongo_db]\n",
    "        self.collection = self.db[self.collection_name]\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        # self.collection.insert_one(dict(item))\n",
    "        query = {'quoteTextHash': dict(item)['quoteTextHash']}\n",
    "        update = {'$set': dict(item)}\n",
    "        self.collection.update_one(query, update, upsert=True)\n",
    "        return item\n",
    "```\n",
    "\n",
    "3. Run `scrapy crawl quotes` to crawl the data and populate into Mongo collection.\n",
    "4. Run below mongo commands to validate. You can also `pymongo` based approach.\n",
    "\n",
    "```js\n",
    "use quotes_db\n",
    "db.quotes.countDocuments({})\n",
    "db.quotes.find({}).pretty()\n",
    "db.quotes.countDocuments({\"pageUrl\": \"https://www.goodreads.com/quotes?page=90\"})\n",
    "db.quotes.find({\"pageUrl\": \"https://www.goodreads.com/quotes?page=90\"}).pretty()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
