{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Review official documentation of Scrapy\n",
    "* Setup Scrapy project using Scrapy CLI\n",
    "* Review Scrapy Project Folder Structure\n",
    "* Add Spider to the Scrapy Project\n",
    "* Update Scrapy Settings to write to json file\n",
    "* Run and Validate the Scrapy Project\n",
    "* Exercise and Solution - Scrape Data to JSON Files"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Review official documentation of Scrapy\n",
    "\n",
    "You can find the complete documentation of Scrapy under https://docs.scrapy.org/en/latest/index.html\n",
    "\n",
    "1. Go through the details related to tutorial\n",
    "2. Understand the code provided in the tutorial (Spider Class, `parse`, `start_urls`, and other important methods)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Setting up Project using Scrapy CLI\n",
    "\n",
    "1. Run `scrapy startproject quotes` to setup scrapy project.\n",
    "2. Check whether a folder by name quotes is not created or not."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Review Scrapy Project Folder Structure\n",
    "\n",
    "1. Check configuration file\n",
    "2. Review `spiders` folder\n",
    "3. Review `settings.py`\n",
    "4. Review `pipelines.py`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Add Spider to the Scrapy Project\n",
    "\n",
    "1. Recreate the project by name quotes_scraper.\n",
    "2. Add a program file under `quotes_scraper/quotes_scraper/spiders` folder by name `quotes.py` with the logic to scrape the data. We can also run `scrapy genspider quotes https://www.goodreads.com/quotes` command to create the spider with boilerplate code. We can also update the created `quotes.py` with the logic to scrape the data.\n",
    "3. Review and understand the code. The code have the ability to process the quotes in all the 100 pages that are available under base url - https://www.goodreads.com/quotes\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "    \n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "\n",
    "    def generate_urls(base_url):\n",
    "        urls = []\n",
    "        for i in range(1, 101):\n",
    "            urls.append(f'{base_url}?page={i}')\n",
    "        return urls\n",
    "\n",
    "    def start_requests(self):\n",
    "        \"\"\"Special method in place of start urls. This will be called automatically to get the list of urls\"\"\"\n",
    "        urls = generate_urls('https://www.goodreads.com/quotes')\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quoteDetails in response.css('.quoteDetails'):\n",
    "            payload = {\n",
    "                'quoteText': quoteDetails.css('.quoteText::text').get(),\n",
    "                'authorOrTitle': quoteDetails.css('span.authorOrTitle::text').get()\n",
    "            }\n",
    "            yield payload\n",
    "```\n",
    "\n",
    "You can also develop the logic to scrape all the pages by following using next_page.\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "    \n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = ['https://www.goodreads.com/quotes?page=90']\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quoteDetails in response.css('.quoteDetails'):\n",
    "            payload = {\n",
    "                'quoteText': quoteDetails.css('.quoteText::text').get(),\n",
    "                'authorOrTitle': quoteDetails.css('span.authorOrTitle::text').get()\n",
    "            }\n",
    "            yield payload\n",
    "\n",
    "        for next_page in response.css('a.next_page'):\n",
    "                yield response.follow(next_page, self.parse)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Update Scrapy Settings to write to json file\n",
    "\n",
    "Make sure to review the details related to the feeds in the official documentation - https://docs.scrapy.org/en/latest/index.html.\n",
    "\n",
    "1. Go to `settings.py` in scrapy project folder.\n",
    "2. Append below text to the file and save it. You can also specify the full path for the output file.\n",
    "\n",
    "```python\n",
    "FEEDS = {\n",
    "    'quotes.json': {\n",
    "        'format': 'json',\n",
    "        'overwrite': True\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Run and Validate the Scrapy Project\n",
    "\n",
    "1. Run `scrapy crawl quotes` to run the spider in the scrapy project.\n",
    "2. Review the data in the files.\n",
    "\n",
    "Note: If you run the project multiple times, the file will be overwritten.\n",
    "\n",
    "Here is the updated code for your reference.\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "    \n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "\n",
    "    def start_requests(self):\n",
    "        \"\"\"Special method in place of start urls. This will be called automatically to get the list of urls\"\"\"\n",
    "\n",
    "        def generate_urls(base_url):\n",
    "            urls = []\n",
    "            for i in range(1, 101):\n",
    "                urls.append(f'{base_url}?page={i}')\n",
    "            return urls\n",
    "        \n",
    "        urls = generate_urls('https://www.goodreads.com/quotes')\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quoteDetails in response.css('.quoteDetails'):\n",
    "            payload = {\n",
    "                'quoteText': quoteDetails.css('.quoteText::text').get(),\n",
    "                'authorOrTitle': quoteDetails.css('span.authorOrTitle::text').get()\n",
    "            }\n",
    "            yield payload\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Exercise - Scrape Data to JSON Files\n",
    "\n",
    "Scrape quote text, author or title, author or title url, author or title url text into JSON format.\n",
    "\n",
    "1. Make sure to have a project by name quotes_scraper.\n",
    "2. Define spider to scrape all the 100 pages.\n",
    "3. Save quote text, author or title, author or title url, and author or title url text details to json file to a file by name `quotes.json`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Solution - Scrape Data to JSON Files\n",
    "\n",
    "Scrape quote text, author or title, author or title url, author or title url text into JSON format.\n",
    "\n",
    "1. Make sure to have a project by name quotes.\n",
    "\n",
    "```shell\n",
    "scrapy startproject quotes_scraper\n",
    "```\n",
    "\n",
    "2. Define spider to scrape all the 100 pages.\n",
    "\n",
    "Create a file by name quotes_spider.py in spiders folder.\n",
    "\n",
    "3. Save quote text, author or title, author or title url, and author or title url text details to json file to a file by name `quotes.json`\n",
    "\n",
    "Update `quotes_spider.py` with below code.\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "    \n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "\n",
    "    def start_requests(self):\n",
    "        \"\"\"Special method in place of start urls. This will be called automatically to get the list of urls\"\"\"\n",
    "\n",
    "        def generate_urls(base_url):\n",
    "            urls = []\n",
    "            for i in range(1, 101):\n",
    "                urls.append(f'{base_url}?page={i}')\n",
    "            return urls\n",
    "        \n",
    "        urls = generate_urls('https://www.goodreads.com/quotes')\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quoteDetails in response.css('.quoteDetails'):\n",
    "            payload = {\n",
    "                'quoteText': quoteDetails.css('.quoteText::text').get(),\n",
    "                'authorOrTitle': quoteDetails.css('span.authorOrTitle::text').get(),\n",
    "                'authorOrTitleUrl': quoteDetails.css('a.authorOrTitle::attr(href)').get(),\n",
    "                'authorOrTitleText': quoteDetails.css('a.authorOrTitle::text').get()\n",
    "            }\n",
    "            yield payload\n",
    "```\n",
    "\n",
    "Make sure below setting is added to `settings.py`.\n",
    "\n",
    "```python\n",
    "FEEDS = {\n",
    "    'quotes.json': {\n",
    "        'format': 'json',\n",
    "        'overwrite': True\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Run `scrapy crawl quotes` to add the data to `quotes.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
